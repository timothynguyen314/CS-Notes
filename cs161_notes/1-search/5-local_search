LOCAL SEARCH

What happens when our environments aren't observable, deterministic, or known?
Local Search evaluates from a current state, rather than systematically from an initial state.
Sometimes paths aren't even relevant, like the N Queens problem, and only the final state matters.
Thus, we don't maintain a path, meaning less memory.

Good for optimization problems, where we want to find the best state according to an objective function.
A state-space landscape associates each state with a value, and there are maximums and minimums along the function.
A complete algorithm always finds a goal.
An optimal algorithm finds the max/min.

Simulated Annealing
	If we never choose a worse move, we can get stuck at a local maximum.
	Simulated Annealing will always choose a move if it's better, but if it's worse, we choose it with a probability that is smaller the worse the decision is.
	We use gradient descent so that we don't go downwards too fast.

Genetic Algorithms
	a beam search generates k random states andd does local search on each of those k states in parallel, having k states in memory instead of just one.
	a stochastic beam search, instead of choosing the best successor from a state, chooses a random successor so that we avoid k processes looking at the same states in parallel.
	a genetic algorithm is a form of stochastic beam search where, instead of generating successors from a single state, generates them by combining parent states

	Starts with k random states, the "population".
	Each state or "individual" is a string of 1s and 0s, with an objective function rating (or a "fitness function").
	States are then paired based on their fitness ratings, and a "crossover point" is chosen for each pair at random.
	Two children are generated by taking bits before and after the crossover point.
	Children are then subject to "mutations" with small independent probability.

	A "schema" is a substring where we only care about certain bits, and the other bits can be anything.
	An "instance" is any string that matches the schema.
	If the average fitness of instances for a certain schema is above the mean, then more instances of that schema will be generated.
	The benefit of genetic algorithms and crossing over only happens if there's meaningful schema

PSEUDOCODE - Simulated Annealing
	simulated_annealing(problem, schedule)
		current = make_node(problem.initial_state)
		for t=1 to infinity
			T = schedule(t)
			if(T == 0)
				return current
			next = randomly selected successor to current
			ΔE = next.value - current.value
			if(ΔE > 0)
				current = next
			else
				current = next w probability e^(ΔE/T)

PSEUDOCODE - Genetic Algorithms
	genetic_algorithm(population, fitness_function)
		while(no individuals are fit enough || enough time has elapsed)
			initialize(new_population)		//empty
			for(i = 1 to population.size)
				x = random_selection(population, fitness_function)
				y = random_selection(population, fitness_function)
				child = reproduce(x, y)
				if(small probability)
					child = mutate(child)
				add child to new_population
			population = new_population
		return best individual in population

	reproduce(x, y)
		n = length(x)
		c = random # from 1 to n
		return append(substring(x, 1, c), substring(y,c+1, n))