WORKING SETS

LRU is successful because the future tends to be like the past, there is usually a degree of temporal/spatial locality. It's pretty much the best strategy. What was used before will be used again, what was used will probably use within the vicinity around it.
But this is for a single process. Multiple processes don't usually use the same page. Global RR and LRU are a bad team.

So we give each process their own set of page frames. We do per-process LRU.

We don't need to give each process as many pages as the size of the virtual address space.
We don't need to give each process as many pages as the full amount it will access.

Too little page frames leads to more page faults faults which is fine, unless we get them due to having too little memory.
Degrading performance due to too little memory is called thrashing.
The ideal time between page faults would be the time slice of RR.

We should have enough memory for all ready processes.
It is better to switch processes in from secondary storage than to have page faults due to too little memory.

"working set size" - the ideal # of page frames where increasing does nothing, but decreasing hurts performance considerably

To find the LRU,
	Each process has an accumulated CPU time.
	Each page frame is associated w an owning process.
	Each page frame has a last referenced time, computed from accumulated CPU time of its owning process.

	We maintain a target age for all pages.
	Pages only age when they are not referenced while their owning process is running.
	If the page is older than the target age, we give it to another process.

dynamic equilibrium/page stealing
	A process will lose pages it does not reference often.
	A process will steal from other processes.
	A process that references more pages more often has larger working sets.
	Instead of adhering to a working set size (which changes with behavior), it simply seeks to reduce page faults and thrashing.